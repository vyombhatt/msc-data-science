{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset being used in this study is pertaining to data collected from edge servers of the Indiana University network. The servers collect the HTTP requests made on the network and report the data back to the holders that maintain the record in raw data files.<br><br>\n",
    "The dataset was available in form of a .tgz file that needed to be read and pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading the tarfile\n",
    "# tar = tarfile.open(\"C:/Users/vyom/Downloads/web-clicks-nov-2009/web-clicks-nov-2009.tar\")\n",
    "# # tar.getmembers()\n",
    "\n",
    "# for member in tar.getmembers():\n",
    "#     f=tar.extractfile(member)\n",
    "#     content=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1243989: expected 11 fields, saw 15\\n'\n",
      "b'Skipping line 1247438: expected 11 fields, saw 15\\n'\n",
      "b'Skipping line 15502608: expected 11 fields, saw 15\\n'\n",
      "b'Skipping line 20842016: expected 11 fields, saw 15\\n'\n",
      "C:\\Users\\vyom\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22133644, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file as csv\n",
    "# This file is currently being loaded from local. Please find the gdrive location for this file in the README file\n",
    "df = pd.read_csv('C:/Users/vyom/Downloads/web-clicks-nov-2009.tgz', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2009-11-01.json</th>\n",
       "      <th>0</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>1,</th>\n",
       "      <th>timestamp:</th>\n",
       "      <th>1257033601,</th>\n",
       "      <th>from:</th>\n",
       "      <th>theybf.com,</th>\n",
       "      <th>to:</th>\n",
       "      <th>w.sharethis.com}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>{\"count\":</td>\n",
       "      <td>1,</td>\n",
       "      <td>timestamp:</td>\n",
       "      <td>1257033601,</td>\n",
       "      <td>from:</td>\n",
       "      <td>,</td>\n",
       "      <td>to:</td>\n",
       "      <td>agohq.org}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>{\"count\":</td>\n",
       "      <td>3,</td>\n",
       "      <td>timestamp:</td>\n",
       "      <td>1257033601,</td>\n",
       "      <td>from:</td>\n",
       "      <td>twistysdownload.com,</td>\n",
       "      <td>to:</td>\n",
       "      <td>adserving.cpxinteractive.com}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>{\"count\":</td>\n",
       "      <td>1,</td>\n",
       "      <td>timestamp:</td>\n",
       "      <td>1257033601,</td>\n",
       "      <td>from:</td>\n",
       "      <td>459.cim.meebo.com,</td>\n",
       "      <td>to:</td>\n",
       "      <td>459.cim.meebo.com}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>{\"count\":</td>\n",
       "      <td>1,</td>\n",
       "      <td>timestamp:</td>\n",
       "      <td>1257033601,</td>\n",
       "      <td>from:</td>\n",
       "      <td>boards.nbc.com,</td>\n",
       "      <td>to:</td>\n",
       "      <td>change.menelgame.pl}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>{\"count\":</td>\n",
       "      <td>1,</td>\n",
       "      <td>timestamp:</td>\n",
       "      <td>1257033601,</td>\n",
       "      <td>from:</td>\n",
       "      <td>mail3-12.sinamail.sina.com.cn,</td>\n",
       "      <td>to:</td>\n",
       "      <td>mail3-12.sinamail.sina.com.cn}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  2009-11-01.json   0  Unnamed: 2   Unnamed: 3     1,  \\\n",
       "0       {\"count\":  1,  timestamp:  1257033601,  from:   \n",
       "1       {\"count\":  3,  timestamp:  1257033601,  from:   \n",
       "2       {\"count\":  1,  timestamp:  1257033601,  from:   \n",
       "3       {\"count\":  1,  timestamp:  1257033601,  from:   \n",
       "4       {\"count\":  1,  timestamp:  1257033601,  from:   \n",
       "\n",
       "                       timestamp: 1257033601,                           from:  \\\n",
       "0                               ,         to:                      agohq.org}   \n",
       "1            twistysdownload.com,         to:   adserving.cpxinteractive.com}   \n",
       "2              459.cim.meebo.com,         to:              459.cim.meebo.com}   \n",
       "3                 boards.nbc.com,         to:            change.menelgame.pl}   \n",
       "4  mail3-12.sinamail.sina.com.cn,         to:  mail3-12.sinamail.sina.com.cn}   \n",
       "\n",
       "  theybf.com,  to: w.sharethis.com}  \n",
       "0         NaN  NaN              NaN  \n",
       "1         NaN  NaN              NaN  \n",
       "2         NaN  NaN              NaN  \n",
       "3         NaN  NaN              NaN  \n",
       "4         NaN  NaN              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dataframe format\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded the data in form of a csv and stored in a dataframe, it is evidently not clean, and needs to be processed. There are 3 major columns of interest which will be picked from the above dataset:\n",
    "- the referer website\n",
    "- the destination website\n",
    "- the count of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns and filtering out the required columns\n",
    "df.columns = range(len(df.columns))\n",
    "dataset = df[[5, 7, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "dataset.columns = ['from_url', 'to_url', 'user_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having picked the relevant columns, we'll cache the data on the local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching the dataframe in local, whcih needs to be cleaned further\n",
    "dataset.to_csv('C:/Users/vyom/Downloads/web-clicks-cleaned.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyom\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# Restarting the kernel, and reading the cached dataframe to proceed with the datacleaning\n",
    "dataset = pd.read_csv('C:/Users/vyom/Downloads/web-clicks-cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the relevant columns, there is plenty of pre-processing that needs to be done. The below sets of codes perform the following types of data cleaning to have the data ready for EDA and modelling:\n",
    "- Some of the websites provided in the dataset are entire URLs of web pages i.e. they include the domain name along with the remaining section of the webpage. Therefore, all sections of the URL are eliminated apart from just the website domain name. This is done by removing all character after (and including) the character ‘/’.\n",
    "- The data consisted of websites from across different countries and organizations. Only the domains with extensions relevant to the UK were considered i.e. domain.com, domain.net, domain.co.uk, domain.edu, domain.org, domain.gov\n",
    "- Filtering out websites that contain punctuation characters that are not associated with the usual format of the websites e.g. ‘%’, ‘/’, ‘\\’, ‘”’, ‘;’, ‘=’.\n",
    "- Filtering out websites that hold explicit/unrestrained content.\n",
    "- Filtering out all the combination of websites that have less than 50 users in common.\n",
    "- Filtering out entries from the dataset that have the same referrer and destination websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the user_count column\n",
    "dataset['user_count'] = dataset['user_count'].str.replace(',', '')\n",
    "dataset = dataset.fillna(0)\n",
    "dataset['user_count'] = dataset['user_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the url columns\n",
    "dataset['from_url'] = dataset['from_url'].str.replace(',', '')\n",
    "dataset['to_url'] = dataset['to_url'].str.replace('}', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows with 0 counts - erronour values\n",
    "dataset = dataset.loc[dataset['user_count']!=0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out for the most common url extensions\n",
    "searchfor = ['.com', '.net', '.co.uk', '.edu', '.org', '.gov']\n",
    "dataset = dataset.loc[dataset['from_url'].str.contains('|'.join(searchfor)), :]\n",
    "dataset = dataset.loc[dataset['to_url'].str.contains('|'.join(searchfor)), :]\n",
    "\n",
    "# Filtering out the url columns that contain data not associated with the format of websites\n",
    "dataset = dataset.loc[dataset['from_url'].str.contains('.', regex=False),:]\n",
    "dataset = dataset.loc[dataset['to_url'].str.contains('.', regex=False),:]\n",
    "\n",
    "dataset = dataset.loc[~dataset['from_url'].str.contains('%'),:]\n",
    "dataset = dataset.loc[~dataset['to_url'].str.contains('%'),:]\n",
    "\n",
    "dataset = dataset.loc[~dataset['from_url'].str.contains('='),:]\n",
    "dataset = dataset.loc[~dataset['to_url'].str.contains('='),:]\n",
    "\n",
    "dataset = dataset.loc[~dataset['from_url'].str.contains('/'),:]\n",
    "dataset = dataset.loc[~dataset['to_url'].str.contains('/'),:]\n",
    "\n",
    "dataset = dataset.loc[~dataset['from_url'].str.contains('\\\\\\\\'),:]\n",
    "dataset = dataset.loc[~dataset['to_url'].str.contains('\\\\\\\\'),:]\n",
    "\n",
    "dataset = dataset.loc[~dataset['from_url'].str.contains(':'),:]\n",
    "dataset = dataset.loc[~dataset['to_url'].str.contains(':'),:]\n",
    "\n",
    "# #Replacing ads. from urls\n",
    "# dataset['from_url'] = dataset['from_url'].str.replace('.*ads.', '')\n",
    "# dataset['to_url'] = dataset['to_url'].str.replace('.*ads.', '')\n",
    "\n",
    "#Replacing www. from urls\n",
    "dataset['from_url'] = dataset['from_url'].str.replace('.*www.', '')\n",
    "dataset['to_url'] = dataset['to_url'].str.replace('.*www.', '')\n",
    "dataset['from_url'] = dataset['from_url'].str.replace('.*www2.', '')\n",
    "dataset['to_url'] = dataset['to_url'].str.replace('.*www2.', '')\n",
    "\n",
    "#Filtering out the apparent explicit websites\n",
    "searchfor = ['xxx', 'xvid', 'porn', 'xtube', 'sex']\n",
    "dataset = dataset.loc[~dataset['from_url'].str.contains('|'.join(searchfor)), :]\n",
    "dataset = dataset.loc[~dataset['to_url'].str.contains('|'.join(searchfor)), :]\n",
    "\n",
    "dataset['from_url'] = dataset['from_url'].str.replace('.com', '.com|').str.split('|').str[0].str.strip()\n",
    "dataset['to_url'] = dataset['to_url'].str.replace('.com', '.com|').str.split('|').str[0].str.strip()\n",
    "dataset['from_url'] = dataset['from_url'].str.replace('.net', '.net|').str.split('|').str[0].str.strip()\n",
    "dataset['to_url'] = dataset['to_url'].str.replace('.net', '.net|').str.split('|').str[0].str.strip()\n",
    "\n",
    "# Keeping only the primary domain name\n",
    "dataset.loc[dataset['from_url'].str.contains('.com'), 'from_url'] = dataset.loc[dataset['from_url'].str.contains('.com'), 'from_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['from_url'].str.contains('.net'), 'from_url'] = dataset.loc[dataset['from_url'].str.contains('.net'), 'from_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['from_url'].str.contains('.gov'), 'from_url'] = dataset.loc[dataset['from_url'].str.contains('.gov'), 'from_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['from_url'].str.contains('.edu'), 'from_url'] = dataset.loc[dataset['from_url'].str.contains('.edu'), 'from_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['from_url'].str.contains('.org'), 'from_url'] = dataset.loc[dataset['from_url'].str.contains('.org'), 'from_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "\n",
    "dataset.loc[dataset['to_url'].str.contains('.com'), 'to_url'] = dataset.loc[dataset['to_url'].str.contains('.com'), 'to_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['to_url'].str.contains('.net'), 'to_url'] = dataset.loc[dataset['to_url'].str.contains('.net'), 'to_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['to_url'].str.contains('.gov'), 'to_url'] = dataset.loc[dataset['to_url'].str.contains('.gov'), 'to_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['to_url'].str.contains('.edu'), 'to_url'] = dataset.loc[dataset['to_url'].str.contains('.edu'), 'to_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "dataset.loc[dataset['to_url'].str.contains('.org'), 'to_url'] = dataset.loc[dataset['to_url'].str.contains('.org'), 'to_url'].apply(lambda x: str(x.split('.')[-2])+'.'+str(x.split('.')[-1]))\n",
    "\n",
    "# Filter out all urls with more than just the domain name\n",
    "dataset['from_domain_sections'] = dataset['from_url'].apply(lambda x: len(x.split('.')))\n",
    "dataset['to_domain_sections'] = dataset['to_url'].apply(lambda x: len(x.split('.')))\n",
    "\n",
    "dataset = dataset.loc[(dataset['from_domain_sections']==2) & (dataset['to_domain_sections']==2), :]\n",
    "\n",
    "dataset = dataset[['from_url', 'to_url', 'user_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = pd.DataFrame(dataset.groupby(['from_url', 'to_url'])['user_count'].sum()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the anomalous websites\n",
    "filtered_dataset = filtered_dataset.loc[filtered_dataset['from_url'].apply(lambda x: len(x))>5,:]\n",
    "filtered_dataset = filtered_dataset.loc[filtered_dataset['to_url'].apply(lambda x: len(x))>5,:]\n",
    "\n",
    "filtered_dataset = filtered_dataset.loc[filtered_dataset['from_url'].apply(lambda x: len(x))<=25,:]\n",
    "filtered_dataset = filtered_dataset.loc[filtered_dataset['to_url'].apply(lambda x: len(x))<=25,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out all the entries that have the same to and from url\n",
    "filtered_dataset = filtered_dataset.loc[filtered_dataset['to_url']!=filtered_dataset['from_url'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = filtered_dataset.loc[filtered_dataset['user_count']>50, :].sort_values('user_count', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having performed the above steps for data cleaning & pre-processing, we're left with a dataset that has 3 columns with no anomalous values in them. This dataset is what will be used going forward for EDA and modelling (done in the next notebook file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_url</th>\n",
       "      <th>to_url</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>740565</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>212049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420365</td>\n",
       "      <td>indianapublicmedia.org</td>\n",
       "      <td>wfiu.org</td>\n",
       "      <td>188708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166391</td>\n",
       "      <td>drudgereport.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>57304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254742</td>\n",
       "      <td>google.com</td>\n",
       "      <td>boost.org</td>\n",
       "      <td>49318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252435</td>\n",
       "      <td>google.com</td>\n",
       "      <td>bio.net</td>\n",
       "      <td>43976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      from_url      to_url  user_count\n",
       "740565             youtube.com  google.com      212049\n",
       "420365  indianapublicmedia.org    wfiu.org      188708\n",
       "166391        drudgereport.com  google.com       57304\n",
       "254742              google.com   boost.org       49318\n",
       "252435              google.com     bio.net       43976"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9729, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset.to_csv('filtered_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
